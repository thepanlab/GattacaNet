{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network on reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pandas_plink import read_plink\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "\n",
    "import random\n",
    "from sklearn.metrics import roc_curve,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Minibatch function'''\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Parameters for experiment '''\n",
    "threshold=\"0.01\"\n",
    "path_logs=\"/work/breastcancer/clean_test/logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' getting bim,fam,bed for training,validation and test sets '''\n",
    "(bim, fam, bed)=read_plink(\"/work/breastcancer/clean_test/train/sig\"+threshold)\n",
    "(bim2, fam2, bed2)=read_plink(\"/work/breastcancer/clean_test/validation/val\"+threshold)\n",
    "(bim3, fam3, bed3)=read_plink(\"/work/breastcancer/clean_test/test/test\"+threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_logs=\"/work/breastcancer/clean_test/logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creating arrays with optimal data structure and filling missing values with 2--> Homozygous major '''\n",
    "print(\"Convertion\")\n",
    "bed=bed.astype('uint8')\n",
    "print(\"Compute\")\n",
    "X=bed.compute()\n",
    "print(\"Filling Null Data\")\n",
    "X[np.isnan(X)]=2\n",
    "#validation\n",
    "print(\"Convertion\")\n",
    "bed2=bed2.astype('uint8')\n",
    "print(\"Compute\")\n",
    "X_val=bed2.compute()\n",
    "print(\"Filling Null Data\")\n",
    "X_val[np.isnan(X_val)]=2\n",
    "#test\n",
    "print(\"Convertion\")\n",
    "bed3=bed3.astype('uint8')\n",
    "print(\"Compute\")\n",
    "X_test=bed3.compute()\n",
    "print(\"Filling Null Data\")\n",
    "X_test[np.isnan(X_test)]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Preparing data.shape=(individuals,SNP) '''\n",
    "#train\n",
    "Y=fam[\"trait\"].astype(\"int\")\n",
    "Y=Y-1\n",
    "Xdf=pd.DataFrame(X.T)\n",
    "Xdf[\"Y\"]=Y\n",
    "\n",
    "#validation\n",
    "Y_val=fam2[\"trait\"].astype(\"int\")\n",
    "Y_val=Y_val-1\n",
    "Xdf_val=pd.DataFrame(X_val.T)\n",
    "Xdf_val[\"Y\"]=Y_val\n",
    "\n",
    "#test\n",
    "Y_test=fam3[\"trait\"].astype(\"int\")\n",
    "Y_test=Y_test-1\n",
    "Xdf_test=pd.DataFrame(X_test.T)\n",
    "Xdf_test[\"Y\"]=Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Getting np arrays '''\n",
    "x_train=Xdf.drop(['Y'],axis=1).values\n",
    "y_train=Xdf[['Y']].values\n",
    "\n",
    "x_val=Xdf_val.drop(['Y'],axis=1).values\n",
    "y_val=Xdf_val[['Y']].values\n",
    "\n",
    "x_test=Xdf_test.drop(['Y'],axis=1).values\n",
    "y_test=Xdf_test[['Y']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' MODEL '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Inputs tensors '''\n",
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(tf.float32,shape=(None,x_train.shape[1]),name=\"X\")\n",
    "Y=tf.placeholder(tf.float32,shape=(None,1),name=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DNN with dropout rate=0.5 and Batch Norm'''\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    hidden00_drop= tf.layers.dropout(X, 0.5, training=training)\n",
    "    hidden01=tf.layers.dense(hidden00_drop, 1000, name=\"hidden01\",activation=None, kernel_initializer=initializer)\n",
    "    hidden01_norm=tf.layers.batch_normalization(hidden01, training=training, momentum=0.9)\n",
    "    act_hidden01=tf.nn.leaky_relu(hidden01_norm)\n",
    "    hidden01_drop = tf.layers.dropout(act_hidden01, 0.5, training=training)\n",
    "    hidden0=tf.layers.dense(hidden01_drop, 250, name=\"hidden0\",activation=None, kernel_initializer=initializer)\n",
    "    hidden0_norm=tf.layers.batch_normalization(hidden0, training=training, momentum=0.9)\n",
    "    act_hidden0=tf.nn.leaky_relu(hidden0_norm)\n",
    "    hidden0_drop = tf.layers.dropout(act_hidden0, 0.5, training=training)\n",
    "    hidden1=tf.layers.dense(hidden0_drop, 50, name=\"hidden1\",activation=None, kernel_initializer=initializer)\n",
    "    hidden1_norm=tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "    act_hidden1=tf.nn.leaky_relu(hidden1_norm)\n",
    "    hidden1_drop = tf.layers.dropout(act_hidden1, 0.5, training=training)\n",
    "    hidden1_norm=tf.layers.batch_normalization(hidden1_drop, training=training, momentum=0.9)\n",
    "    output=tf.layers.dense(  hidden1_norm, 1, name=\"output_final\",activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Log-Loss '''\n",
    "with tf.name_scope(\"loss\"):\n",
    "    l2_regularizer = tf.contrib.layers.l2_regularizer(scale=0.01, scope=None)\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=output)\n",
    "    weights = tf.trainable_variables()\n",
    "    loss=tf.reduce_mean(cross_entropy)\n",
    "    error=loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Adam Optimizer '''\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer =tf.train.AdamOptimizer(learning_rate=0.0001,beta1=0.9,beta2=0.999,epsilon=1e-08,use_locking=False,name='Adam')\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Metrics '''\n",
    "with tf.name_scope(\"eval\"):\n",
    "    predicted = tf.nn.sigmoid(output)\n",
    "    correct_pred = tf.equal(tf.round(predicted), Y)\n",
    "    acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    _,auc = tf.metrics.auc(labels=Y,predictions=predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Init model '''\n",
    "init=tf.global_variables_initializer()\n",
    "loc=tf.local_variables_initializer()\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(device_count={ \"CPU\": 44}))\n",
    "init.run()\n",
    "loc.run()\n",
    "auc_tab=[]\n",
    "accuracy_tab=[]\n",
    "loss_tab=[]\n",
    "epoch_tab=[]\n",
    "auc_tab_val=[]\n",
    "accuracy_tab_val=[]\n",
    "loss_tab_val=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "batch_size=512\n",
    "best_auc=0\n",
    "open(path_logs+'Test_preprocessed_filter'+threshold+'.txt', 'w')\n",
    "for epoch in range(200):\n",
    "    iteration=0\n",
    "    for x_batch,y_batch in shuffle_batch(x_train, y_train, batch_size):\n",
    "        sess.run(training_op,feed_dict={X:x_batch,Y:y_batch,training:True})\n",
    "        print(\"%d ITERATION:%d/%d \"%(epoch,iteration,len(x_train)//batch_size),end='\\r')\n",
    "        iteration+=1\n",
    "        \n",
    "    loc.run()\n",
    "    loss_train,acc_train,auc_train=sess.run([loss,acc,auc],feed_dict={X:x_batch,Y:y_batch,training:False})\n",
    "    print(epoch,\"Train accuracy:\",acc_train,\"Loss:\",loss_train,\"AUC:\",auc_train)\n",
    "    with open(path_logs+'Test_preprocessed_filter'+threshold+'.txt', 'a+') as file:\n",
    "        file.write(\"Epoch\"+str(epoch)+\" Training accuracy:\"+str(acc_train)+\" Loss:\"+str(loss_train)+\" AUC:\"+str(auc_train)+\"\\n\\n\")\n",
    "    #train value\n",
    "    auc_tab.append(auc_train)\n",
    "    accuracy_tab.append(acc_train)\n",
    "    epoch_tab.append(epoch)\n",
    "    loss_tab.append(loss_train)\n",
    "    #validation\n",
    "    loc.run()\n",
    "    loss_val,acc_val,auc_val=sess.run([loss,acc,auc],feed_dict={X:x_val,Y:y_val,training:False})\n",
    "    auc_tab_val.append(auc_val)\n",
    "    accuracy_tab_val.append(acc_val)\n",
    "    loss_tab_val.append(loss_val)\n",
    "    if best_auc<auc_val:\n",
    "        saver.save(sess,path_logs+\"preprocessed\"+threshold+\".ckpt\")\n",
    "        best_auc=auc_val\n",
    "    with open(path_logs+'Test_preprocessed_filter'+threshold+'.txt', 'a+') as file:\n",
    "        file.write(\"Epoch\"+str(epoch)+\" Validation accuracy:\"+str(acc_val)+\" Loss:\"+str(loss_val)+\" AUC:\"+str(auc_val)+\"\\n\\n\")\n",
    "    print(epoch,\"Validation accuracy:\",acc_val,\"Loss:\",loss_val,\"AUC:\",auc_val)\n",
    "    print(\"\\n\")\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Max AUC validation '''\n",
    "np.max(auc_tab_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Max AUC validation epoch'''\n",
    "np.argmax(auc_tab_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training time '''\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Max AUC validation model retrieving'''\n",
    "sess.close()\n",
    "init=tf.global_variables_initializer()\n",
    "loc=tf.local_variables_initializer()\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(device_count={ \"CPU\": 44}))\n",
    "init.run()\n",
    "loc.run()\n",
    "saver.restore(sess,path_logs+\"preprocessed\"+threshold+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Predictions on validation set '''\n",
    "pred=sess.run(predicted,feed_dict={X:x_val,Y:y_val,training:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Predictions on test set '''\n",
    "pred_test=predicted.eval(feed_dict={X:x_test,Y:y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Score metrics on validation set '''\n",
    "loc.run()\n",
    "sess.run([loss,acc,auc],feed_dict={X:x_val,Y:y_val,training:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Score metrics on test set '''\n",
    "loc.run()\n",
    "sess.run([loss,acc,auc],feed_dict={X:x_test,Y:y_test,training:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Score metrics on test set computed with sklearn.metrics.roc_auc_score'''\n",
    "roc_auc_score(y_test[:,0], pred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot AUC and accuracy over learning '''\n",
    "#AUC\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "ax.plot(epoch_tab,auc_tab,label='AUC')\n",
    "ax.plot(epoch_tab,auc_tab_val,label='AUC Validation p='+thereshold+' Dataset')\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"Metrics' values\")\n",
    "ax.set_title('Training auc vs epoch full p='+thereshold+' Dataset')\n",
    "legend = ax.legend(loc='upper right', shadow=True, fontsize='x-small')\n",
    "plt.savefig(path_logs+'auc_'+threshold+'.png')\n",
    "\n",
    "#Accuracy\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "ax.plot(epoch_tab,accuracy_tab,label='Accuracy')\n",
    "ax.plot(epoch_tab,accuracy_tab_val,label='Accuracy Validation Full Dataset')\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"Metrics' values\")\n",
    "ax.set_title('Training accuracy vs epoch p='+thereshold+' Dataset')\n",
    "legend = ax.legend(loc='upper right', shadow=True, fontsize='x-small')\n",
    "plt.savefig(path_logs+'acc_'+threshold+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Plot prediction histogram on test set  '''\n",
    "results=pd.DataFrame(pred_test)\n",
    "target=pd.DataFrame(y_test)\n",
    "target.columns=[\"Prediction Target\"]\n",
    "result_pred=pd.concat([results,target],axis=1)\n",
    "result_pred.columns=[\"Predicted\",\"Prediction Target\"]\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "r=result_pred.groupby('Prediction Target')['Predicted']\n",
    "r.plot(kind='hist',alpha=.4,legend=True)\n",
    "ax.set_xlabel(\"Probabilities\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title('Histogram of the predicted values Full Dataset')\n",
    "plt.savefig(path_logs+'histprediction_'+threshold+'.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
